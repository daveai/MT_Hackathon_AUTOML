{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mind Tourism Hackathon - Classification",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CTeRcR3P26H"
      },
      "source": [
        "# TPOT Classification Tutorial\n",
        "\n",
        "In this tutorial we’ll explore how to use tpot to automatically tune ML models to our data. In this tutorial we’ll be looking at classification models, tpot also supports regression for more details on regression please see the documentation at https://epistasislab.github.io/tpot/.\n",
        "\n",
        "First we’ll import the necessary packages as well as the data we want to work with. Please note the notebook below is a Python 3.x notebook. Basic familiarity with Python is therefore assumed. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR8U_52Oyiuv"
      },
      "source": [
        "##############################################\n",
        "##############   SETTINGS   ##################\n",
        "\n",
        "DATASET_NAME = \"data.csv\"\n",
        "TARGET_VARIABLE = \"REPEATER\"\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "GENERATIONS = 10\n",
        "POPULATION = 10\n",
        "CROSSVALIDATION_SPLIT = 3\n",
        "\n",
        "##############################################\n",
        "##############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_gIhkWTSsDU"
      },
      "source": [
        "# <--- Press play for step one:\n",
        "# Installs the TPOT AutoML library as well as its dependecies\n",
        "!pip install -q deap update_checker tqdm stopit tpot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_ptqg--Q4Si"
      },
      "source": [
        "The code below will download our sample dataset. To load your own dataset, you can simply upload a csv file called \"dataset.csv\". In case of uploading your own dataset, you don't need to run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlLxHjwQQpKu"
      },
      "source": [
        "# <--- Press play:\n",
        "# Downloads the sample dataset.\n",
        "# If you want to upload your own dataset, please use the upload\n",
        "# folder on the left-hand side of Google Colab. For simplicity upload\n",
        "# your data as a csv file.\n",
        "!wget [EXCLUDED FOR DATA PRIVACY REASONS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PWFGmEPQJ5F"
      },
      "source": [
        "We now have to separate our target variable from the rest of the dataset. Our target variable, is the one we want our model to predict - in our case regress. We’ll use the conventional X, y notation as common in the ML community. X is the matrix of predictor variables, while y is the target variable. The outputs of our model will be the ŷ (y hat). \n",
        "\n",
        "With our data separated, we can use the sklearn library to easily split our data into training and test sets. The train_test_split class simply takes some data and splits it into 4 sets: X_train, X_test, y_train, y_test. The purpose of the test set is to later assess our model on data it has not seen beforehand, thereby attempting to get an accurate measure of the ability of our model to generalise on unforeseen data. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejDCs3pqQJZ3"
      },
      "source": [
        "# <--- Press play:\n",
        "\n",
        "# Imports the modules needed\n",
        "from tpot import TPOTClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the dataset.csv file. In the case of uploading\n",
        "# an excel file, it is possible to read the data by using\n",
        "# pd.read_excel(\"dataset.xlsx\")\n",
        "X = pd.read_csv(DATASET_NAME)\n",
        "X = X.select_dtypes([np.number])\n",
        "X.dropna(inplace=True)\n",
        "\n",
        "# Uses the \"target\" column of the data as the target variable\n",
        "# make sure your target variable is called target, or change\n",
        "# it below to your target variable.\n",
        "y = X[TARGET_VARIABLE]\n",
        "X.drop(TARGET_VARIABLE, axis=1, inplace=True)\n",
        "\n",
        "# Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdVoOf9BRRGk"
      },
      "source": [
        "To initialize tpot we need to define a number of parameters. For a full and detailed overview of all parameters please see the documentation at https://epistasislab.github.io/tpot/api/#regression.  \n",
        "\n",
        "Parameters set below:  \n",
        "- generations: defines the number of generations (iterations) to run the search process.  \n",
        "- population size: defines the number of individuals (models) in each generation.  \n",
        "- scoring: the objective function, in our case negative mean squared error.  \n",
        "- cv: how many splits to use for cross-validation.  \n",
        "- n jobs: how many cores of your CPU to use. -1 will use all cores, -2 all but one core.  \n",
        "- random state: seed for reproducability.  \n",
        "- verbosity: defines the output of the model during the training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8MwosGlRR8K"
      },
      "source": [
        "# <--- Press play:\n",
        "\n",
        "# Initializes tpot\n",
        "tpot = TPOTClassifier(generations=GENERATIONS, population_size=POPULATION,\n",
        "                      cv=CROSSVALIDATION_SPLIT, verbosity=2, n_jobs=-1,\n",
        "                      periodic_checkpoint_folder='/content/results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZyVKJHRTSq"
      },
      "source": [
        "While in the above section we simply defined the parameters of tpot we now call fit on it and pass the training data to it. This will start exploring models and hyperparameters - the amount of training depends on the parameters set above. For smaller datasets, a couple of hours of search can already find good performing models. Yet, it is norm in the ML community to run automl for a couple of days or even longer - depinding on the data we are working with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rZAH4iQS_0j"
      },
      "source": [
        "# <--- Press play:\n",
        "# Starts the search for models (will take some time)\n",
        "tpot.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cWlwSt_RWHC"
      },
      "source": [
        "Please note that for the purpose of this tutorial we only ran tpot for roughly ~5 minutes. As already mentioned above, the typical range for searching models can range anywhere from 2-3 hours up to 48-72 and beyond that for large, complex datasets. To increase the search time in tpot one should increase the number of generations, population size will also increase the search space and time of tpot. tpot automatically prints the best pipeline and parameters at the end of a run.\n",
        "\n",
        "Once our training process has ended, our tpot class itself can be used as a model / ensemble to predict. In fact, we can use the class to predict our error rate on the test data we split initially to see how well it performs on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR74qqrxRx2y"
      },
      "source": [
        "# <--- Press play:\n",
        "\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "y_pred = tpot.predict(X_test)\n",
        "\n",
        "print(\"RESULTS OF BEST MODEL:\\n\")\n",
        "print(f\"Accuracy:             {skm.accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Balanced Accuracy:    {skm.balanced_accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"F1 Score:             {skm.f1_score(y_test, y_pred, average='weighted')}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWQGIeZGtb8W"
      },
      "source": [
        "skm.confusion_matrix(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K5kNZEmS_AY"
      },
      "source": [
        "We can export our best model by running the code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tK2FPw2X3Qz"
      },
      "source": [
        "individuals = tpot.evaluated_individuals_\n",
        "pareto_indiv = tpot.pareto_front_fitted_pipelines_\n",
        "dict_keys = list_models = [i for i in pareto_indiv]\n",
        "\n",
        "accuracy = []\n",
        "model_name = []\n",
        "model_params = []\n",
        "\n",
        "for model in dict_keys:\n",
        "  accuracy.append(individuals.get(dict_keys[0])['internal_cv_score'])\n",
        "  model_name.append(pareto_indiv[model].steps[-1][0])\n",
        "  model_params.append(pareto_indiv[model].steps[-1])\n",
        "\n",
        "print(f\"Total models evaluated: {len(individuals)}\")\n",
        "\n",
        "df = pd.DataFrame(list(zip(accuracy, model_name, model_params)), columns=['CV_Accuracy', 'Model', 'Model_Params'])\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}