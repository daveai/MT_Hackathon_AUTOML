{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mind Tourism Hakcathon - Regression",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYNFolMrUOte"
      },
      "source": [
        "# TPOT Regression Tutorial\n",
        "\n",
        "In this tutorial we’ll explore how to use tpot to automatically tune ML models to our data. In this tutorial we’ll be looking at regression models, tpot also supports classification for more details on classification please see the documentation at https://epistasislab.github.io/tpot/.\n",
        "\n",
        "First we’ll import the necessary packages as well as the data we want to work with. Please note the notebook below is a Python 3.x notebook. Basic familiarity with Python is therefore assumed. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPMJRMm7azb4"
      },
      "source": [
        "##############################################\n",
        "##############   SETTINGS   ##################\n",
        "\n",
        "DATASET_NAME = \"vgdata.csv\"\n",
        "TARGET_VARIABLE = \"revenue\"\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "GENERATIONS = 10\n",
        "POPULATION = 10\n",
        "CROSSVALIDATION_SPLIT = 3\n",
        "\n",
        "##############################################\n",
        "##############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX2hd946UKyR"
      },
      "source": [
        "# Installs the TPOT AutoML library as well as its dependecies\n",
        "!pip install deap update_checker tqdm stopit tpot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Jaf7hHUXhQ"
      },
      "source": [
        "The code below will download our sample dataset. To load your own dataset, you can simply upload a csv file called \"dataset.csv\". In case of uploading your own dataset, you don't need to run the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5qZQFOZUZhW"
      },
      "source": [
        "# Downloads our hotel dataset (our sample dataset)\n",
        "# If you want to upload your own dataset, please use the upload\n",
        "# folder on the left-hand side of Google Colab. For simplicity upload\n",
        "# your data as a csv file.\n",
        "!wget [EXCLUDED FOR DATA PRIVACY REASONS]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P1QbnEiUlD4"
      },
      "source": [
        "We now have to separate our target variable from the rest of the dataset. Our target variable, is the one we want our model to predict - in our case regress. We’ll use the conventional X, y notation as common in the ML community. X is the matrix of predictor variables, while y is the target variable. The outputs of our model will be the ŷ (y hat). \n",
        "\n",
        "With our data separated, we can use the sklearn library to easily split our data into training and test sets. The train_test_split class simply takes some data and splits it into 4 sets: X_train, X_test, y_train, y_test. The purpose of the test set is to later assess our model on data it has not seen beforehand, thereby attempting to get an accurate measure of the ability of our model to generalise on unforeseen data. \n",
        "\n",
        "NOTE: TPOT WORKS WITH NUMERIC DATASETS. If your dataset has categorical features you will have to transform them into numerical ones before uploading your dataset. The target variable must also be numeric!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKisAdvqUeEi"
      },
      "source": [
        "# Imports the packages used throughout the notebook, also imports \n",
        "# TPOT (which we installed in step 1)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Read the dataset.csv file. In the case of uploading\n",
        "# an excel file, it is possible to read the data by using\n",
        "# pd.read_excel(\"dataset.xlsx\").\n",
        "\n",
        "X = pd.read_csv(DATASET_NAME)\n",
        "X = X.select_dtypes([np.number])\n",
        "X.dropna(inplace=True)\n",
        "\n",
        "y = X[TARGET_VARIABLE]\n",
        "X.drop([TARGET_VARIABLE], axis=1, inplace=True)\n",
        "\n",
        "# Splits the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeI43zliipuh"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijte3h-NUx3f"
      },
      "source": [
        "To initialize tpot we need to define a number of parameters. For a full and detailed overview of all parameters please see the documentation at https://epistasislab.github.io/tpot/api/#regression.  \n",
        "\n",
        "Parameters set below:  \n",
        "- generations: defines the number of generations (iterations) to run the search process.  \n",
        "- population size: defines the number of individuals (models) in each generation.  \n",
        "- scoring: the objective function, in our case negative mean squared error.  \n",
        "- cv: how many splits to use for cross-validation.  \n",
        "- n jobs: how many cores of your CPU to use. -1 will use all cores, -2 all but one core.  \n",
        "- random state: seed for reproducability.  \n",
        "- verbosity: defines the output of the model during the training.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9GJG_iUnxc"
      },
      "source": [
        "# <--- Press play:\n",
        "\n",
        "# initializing tpot with parameters\n",
        "tpot = TPOTRegressor(generations=GENERATIONS, population_size=POPULATION,\n",
        "                     cv=CROSSVALIDATION_SPLIT, verbosity=2, n_jobs=-1,\n",
        "                     periodic_checkpoint_folder='/content/results')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlZZyKZdU2wP"
      },
      "source": [
        "While in the above section we simply defined the parameters of tpot we now call fit on it and pass the training data to it. This will start exploring models and hyperparameters - the amount of training depends on the parameters set above. For smaller datasets, a couple of hours of search can already find good performing models. Yet, it is norm in the ML community to run automl for a couple of days or even longer - depinding on the data we are working with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI07xIYmU44J"
      },
      "source": [
        "# <--- Press play:\n",
        "\n",
        "# Starts the training / search for models\n",
        "tpot.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3aM8pRtU66j"
      },
      "source": [
        "Please note that for the purpose of this tutorial we only ran tpot for roughly ~5 minutes. As already mentioned above, the typical range for searching models can range anywhere from 2-3 hours up to 48-72 and beyond that for large, complex datasets. To increase the search time in tpot one should increase the number of generations, population size will also increase the search space and time of tpot. tpot automatically prints the best pipeline and parameters at the end of a run.\n",
        "\n",
        "Once our training process has ended, our tpot class itself can be used as a model / ensemble to predict. In fact, we can use the class to predict our error rate on the test data we split initially to see how well it performs on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU9qvgADU88j"
      },
      "source": [
        "import sklearn.metrics as skm\n",
        "\n",
        "y_pred = tpot.predict(X_test)\n",
        "\n",
        "print(\"RESULTS OF BEST MODEL:\\n\")\n",
        "\n",
        "print(f\"R2:                         {skm.r2_score(y_test, y_pred)}\")\n",
        "print(f\"Mean Squared Error:         {skm.mean_squared_error(y_test, y_pred)}\")\n",
        "print(f\"Root Mean Squared Error:    {skm.mean_squared_error(y_test, y_pred, squared=False)}\")\n",
        "print(f\"Mean Absolute Error:        {skm.mean_absolute_error(y_test, y_pred)}\")\n",
        "print(f\"Explained Variance:         {skm.explained_variance_score(y_test, y_pred)}\")\n",
        "print(f\"Max Error:                  {skm.max_error(y_test, y_pred)}\")\n",
        "print(f\"Median Absolute Error       {skm.median_absolute_error(y_test, y_pred)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjGtRGUTVCh_"
      },
      "source": [
        "We can export our best model by running the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru00frsBVEaX"
      },
      "source": [
        "individuals = tpot.evaluated_individuals_\n",
        "pareto_indiv = tpot.pareto_front_fitted_pipelines_\n",
        "dict_keys = list_models = [i for i in pareto_indiv]\n",
        "\n",
        "error = []\n",
        "model_name = []\n",
        "model_params = []\n",
        "\n",
        "for model in dict_keys:\n",
        "  error.append(individuals.get(dict_keys[0])['internal_cv_score'])\n",
        "  model_name.append(pareto_indiv[model].steps[-1][0])\n",
        "  model_params.append(pareto_indiv[model].steps[-1])\n",
        "\n",
        "print(f\"Total models evaluated: {len(individuals)}\")\n",
        "\n",
        "df = pd.DataFrame(list(zip(error, model_name, model_params)), columns=['Neg_Mean_Squared_Error', 'Model', 'Model_Params'])\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}